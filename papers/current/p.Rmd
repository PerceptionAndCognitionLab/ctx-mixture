---
title             : "Some do and some don't? Accounting for possible variation in strategies."
shorttitle        : "Variation in strategies."

author:
  - name          : "Julia M. Haaf"
    affiliation   : "1"
    corresponding : yes
    address       : "205 McAlester Hall"
    email         : "JHaaf@mail.missouri.edu"
  - name          : "Jeffrey N. Rouder"
    affiliation   : "1, 2"

affiliation:
  - id            : "1"
    institution   : "University of Missouri"
  - id            : "2"
    institution   : "University of California, Irvine"

author_note: >
  This paper was written in R-Markdown with code for data analysis integrated into the text.  The Markdown script is open and freely available at [https://github.com/PerceptionAndCognitionLab/ctx-mixture](https://github.com/PerceptionAndCognitionLab/ctx-mixture). The data used here are not original.  We make these freely available with permission of the original authors at [https://github.com/PerceptionCognitionLab/data0/tree/master/contexteffects](https://github.com/PerceptionCognitionLab/data0/tree/master/contexteffects).

abstract: >
  A primary question in experimental psychology is whether different people use different strategies when performing a task. In this paper, we focus on priming and context tasks where different strategies lead to different outcomes or effects. Consider, for example, a Stroop task where the target word is presented in the visual periphery. A strategy-pure case is when all people read the word quickly and automatically such that word identity interferes with color naming. A strategy-mixed case is as follows: Some people may choose to make an eye movement, read the word, and exhibit a Stroop effect; others may not make an eye movement, identify the color without reading, and show no Stroop effect. We define strategies as tied to ordinal relations and constraints among outcomes, and develop a family of Bayesian hierarchical models that capture a variety of these constraints. Doing so leads to new definitions of heterogeneity where some variation across people is due to variation within a common strategy, while other variation across people is due to variation of strategies. We apply this approach to Stroop interference experiments, and a near-liminal priming experiment where the prime may be below and above threshold for different people.

keywords          : "Cognitive psychometrics, Individual differences, Bayes factors, Mixture models"

bibliography      : ["r-references.bib", lab.bib]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
header-includes:
   - \usepackage{bm}
   - \usepackage{pcl}
   - \usepackage{amsmath}
   - \usepackage{setspace}
   - \usepackage{bm}
output            : papaja::apa6_pdf
csl               : apa6.csl
---

```{r include = FALSE}
library("papaja")
library("spatialfil")
library("tmvtnorm")
library("msm")
library(curl)
library(devtools)
library(BayesFactor)
library("MCMCpack")
library("diagram")
library("plotrix")
library("colorspace")
# library("beeswarm")

# SourceURL <- "https://raw.githubusercontent.com/PerceptionAndCognitionLab/ctx-indiff/public/shared/functions/modelcomp.R"
# source_url(SourceURL)
my_citation <- cite_r(file = "r-references.bib")

source("../../shared/modelcomp.R")
source("../../shared/figHelp.R")

chains = T
knitr::opts_chunk$set(warning = FALSE)
```

*Some of them want to use you. Some of them want to get used by you.* [@Eurythmics:1983]

A prevailing folk wisdom is that different people do things differently. In cognitive sciences this folk wisdom manifests in the concept of *strategy* as being distinct from the concept of *process*.
Process refers to a series of cognitive operations that a participant may go through in completing a task. Strategy, on the other hand, implies that the participant may have several different processing paths to complete a task and selects among these. Take, for example, the learning of novel alphabet arithmetic statements such as "A + 3 = D". Useful processes for this task may be counting and recollection from memory [@Logan:1988]. We say there are multiple strategies in play if participants use these two processes in different configurations. For example, there may be four different strategies: 1. A participant may solely use counting; 2. A participant may solely use recollection; 3. A participant may use one or the other on any given trial [@Rickard:2004]; 4. A participant may use both on every trial, say in a race configuration [@Logan:1992]. If every participant uses the same strategy, we call the task a strategy-pure task. Alternatively, if different participants engage in different strategies, we call the task a strategy-mixed task. We note here that the choice of strategy need not refer to a conscious act. People may automatically rely on one strategy or another.

As researchers, we are sometimes critiqued for not accounting for variations in strategies. For example, if we propose a race model for alphabet arithmatic, a reviewer might ask if all people race. This is a difficult critique, especially if reviewers assume *a priori* that multiple strategies exist. The critique raises the important question of how to tell whether a task is strategy-pure or strategy-mixed. And having a principled means of ruling in or ruling out the possiblity of multiple strategies across individuals would be a generally applicable advance for theory development in cognitive psychology.  Our goal here is just this---to present a means of addressing the multiple-strategy question across individuals.

Providing empirical evidence for mixtures of strategies is a complicated task. Certainly, there will be cases where it is nearly impossible to do so. For instance, consider two very simiar processing models, say the diffusion model [@Ratcliff:1978] and the linear ballistic accumulator model [@Brown:Heathcote:2008]. It strikes us as impossible to detect mixtures of these models across people.

Our approach here is to map simple behavioral outcomes to processing. This mapping is easiest in the context of specific tasks rather than in general.  Consider a priming task, for example.  In the task, participants are flashed a prime before identifying a subsequent target stimulus. If prime and target share features, that is if they are compatible, then we typically observe a reduction in the time to identify the target compared to incompatible primes. Because this is the typical effect, we refer to it as the positive priming effect. In contrast, a negative priming effect occurs when incompatible primes speed the identification of a target.

To define the strategies that could be in play here, we identify three outcomes:  The first outcome is the positive effect, and it is compatible with the prime activating the target.  The second outcome is the negative effect.  If it occured here, one would think of some contrast effect.  The third outcome is no priming effect, and the strategy is that of successful segregation and supression of the prime.  Now we are ready to define strategy-pure and strategy-mixed tasks. The priming task is strategy-pure if all participants show the same effect---that is all display true negative priming, all display a lack of priming, or all display true positive priming.  A priming task is strategy-mixed if there are differences in the sign of the true effects.  Perhaps some participants have a true null effect while others have a true positive effect.

The question then is how can we assess evidence for strategy-pure and strategy-mixed cases when outcomes map to processing? There are two important considerations in pursuing this question.  The first is the difference between observed and true effects. The second is the difference between variation within a process and variation across processes. We take these in turn:

Understanding the distinction between observed and true effects is essential. We can certaintly graph observed effects for all individuals in any task such as the aforementioned priming task and determine whether everyones' observed effects are positive, negative or null. This procedure, however, does not solve the issue as the concept of measurement noise is disregarded. Statements about pure and mixed strategies therefore have to refer to true effects, and a model that incorporates noise is needed for inference.

The priming example also illustrates the second consideration of individual variability. The distinction between variability within a process and across processes is necessary in this setup. It is plausible that individuals' effects may vary in size, say, one individual may have a true $30$ ms priming effect while another may have a true $300$ ms priming effect. This variability is an indicator for heterogeneity within a process. It does not, however, imply mixed strategies. On the other hand, if one individual has a true effect of $-30$ ms (indicating a negative priming effect) and another individual has a true effect of $30$ ms, then the task is strategy-mixed. Variation alone is not critical; The direction of effects is.

# Stating evidence for strategies

```{r classification, fig.cap = "Individual observed effects from a priming task ordered from lowest to highest. Shading of the points indicates strategy according to two criteria. Dark blue points indicate a positive priming effect for the criterion of BF>2. Light or dark blue points indicate a positive priming effect for the criterion of 80%CIs excluding zero. White points indicate a null effect according to both criteria."}
sourceDat <- "https://raw.githubusercontent.com/PerceptionCognitionLab/data0/master/contexteffects/numberSubPriming/numberSubPriming.R"
source_url(sourceDat)

make.cis <- function(dat){
    rts <- split(dat$rt, dat$cond)
    t <- t.test(rts$`0`, rts$`1`, conf.level = 0.8)
    bf <- ttestBF(x = rts$`0`, y = rts$`1`, nullInterval = c(0, Inf))
    c((t$estimat[1]-t$estimat[2]), t$conf.int[1:2], extractBF(bf)[1,1])
}

datclean$condn <- 1 - datclean$cond
I <- length(unique(datclean$sub))
sub <- as.factor(datclean$sub)
levels(sub) <- 1:I

datclean$condn <- factor(datclean$condn, labels = c("c","i"))
delta <- matrix(unlist(by(datclean, sub, make.cis, simplify = TRUE))
                , ncol = 4, byrow = T)

delta_i <- delta[order(delta[, 1]),]

par(mfrow = c(1,1), mar=c(4,4.5,1,1), mgp = c(2.2,1,0))
  #PLOT DELTA WITH REJECTED NULL
  plot(x = 1: nrow(delta_i)
       , y = delta_i[, 1]
       , ylim = c(min(delta_i[, 2]), max(delta_i[, 3]))
       # , pch = 19
       , col = "gray40"
       , ylab = expression(paste("Observed effect ", d[i], " (sec)"))
       , xlab = "Participants"
       , frame.plot = FALSE
       , axes = FALSE
  )
abline(h = 0, col = "gray60", lwd = 1.5)
  
plotCI(x = 1: nrow(delta_i)
       , y = delta_i[, 1]
       , ui= delta_i[, 3]
       , li= delta_i[, 2]
       , add = TRUE
       # , sfrac = 1/150
       # , pch = 19
       , col = "gray40"
       , pch = 21
       , pt.bg=par("bg")
       , cex = 1.2)

points(x = 1: nrow(delta_i)
       , y = delta_i[, 1]
       , pch = 19
       , col = "white")

axis(side = 1
     , at = c(1, nrow(delta_i)))
axis(side = 2
    , at = seq(-.05, .05, .025))

ind <- delta_i[, 2] > 0
goodCI <- delta_i[ind,]
points(x = (1:I)[ind]
       , y <- goodCI[, 1]
       , pch = 19
       , col = "cornflowerblue")

ind2 <- delta_i[, 4] > 1
goodBF <- delta_i[ind2, ]
points(x = (1:I)[ind2]
       , y = goodBF[, 1]
       , pch = 19
       , col = "navy")

points(x = 1: nrow(delta_i)
       , y = delta_i[, 1]
       , col = "gray40")
```

The strategy question belies a difficult conceptual issue---how to account for parsimony. While it is easy to wonder about multiple strategies, it is critical to realize a multiple strategy account may add unneeded complexity.  The simplest explanation of a phenomenon is that all people use the same strategy. In this case, the lack of variation in process leads to simple theories that apply to all people. The more complicated explanation is that there is variation in strategies across individuals. One must not only specify multiple processing possibilities but explane why certain individuals follow one strategy while others follow different strategies. Even though the strategy-pure explanantion is simper, it is our experience that researchers are often quick to make a verbal argument for mixed strategies. Indeed, we are reminded of a well-known quote commonly attributed to Einstein: "Everything should be made as simple as possible, but not simpler".^[See [http://quoteinvestigator.com/2011/05/13/einstein-simple/](http://quoteinvestigator.com/2011/05/13/einstein-simple/) for a discussion on the attribution of the quote.]

We argue that whether mixed strategies should be incorporated in a theoretical model is an empirical question. A precedented way of evidencing such mixed strategies is to classify individuals into different modes of processing. A good example comes from @Little:etal:2011 who classified individuals as using serial, parallel or coactive processing based on the direction and magnitude of an interaction contrast. Figure\ \@ref(fig:classification) illustrates this classification method. The figure shows the ordered effects of each individual in a priming task.^[Data comes from @Pratte:Rouder:2009, Experiment 2. Details on the data set can be found in the Application section.] As can be seen, a few people show a negative sample effect, many people have near-zero sample effects, and a few have substantial positive sample effects. One way of classifying people is to draw a confidence interval around each sample effect. In the figure we use the 80%CIs to balance error rates. According to this classification scheme, `r I - sum(ind)` of the `r I` individuals show an effect that is not distiguishable from zero (open dots), and `r sum(ind)` individuals show a positive effect. Another classification scheme is to compute Bayes factors for each indivdual. Figure\ \@ref(fig:classification) highlights the four individuals' effects that are more compatible with an effects-model than with the null. According to both approaches, this priming task is strategy-mixed with some people showing an effect and others not. There have been several mixture model approaches in psychology with the goal of classification [e.g. @Houpt:Fific:2017; @Little:etal:2011; @Rouder:etal:2007b].

@Thiele:etal:2017 level a difficult critique at using classification schemes to assess the strategy question. In classification, sample noise is misinterpreted as evidence for the mixed-strategy conclusion. Heterogeneity is assumed, and nuisance variation may result in both null and effect interpretations. In fact, with an appropriate analysis to be presented subsequently, we show that the data in Figure\ \@ref(fig:classification) are best understood as a single, small, common effect with no variation across people.

To mitigate Thiele et al.\'s critique, @Haaf:Rouder:2017 and @Thiele:etal:2017 developed a Bayesian model-comparison framework where some models are explicitly strategy-pure. In this framework, a single model is placed on the collection of individuals\' effects rather than placing separate models on each individual's effect. Take, for example, the strategy-pure model where all individuals have a positive true priming effect. This model is implemented by simulaneously imposing order constraints---effects must be positive---on all individuals. How to assess all these order-constraints simultaneously is not known in conventional statistical frameworks. Fortunately, this assessment is conceptually straightforward in the Bayesian framework [@Gelfand:etal:1992], and computational development is provided in @Haaf:Rouder:2017 and @Klugkist:etal:2005. This Bayesian framework provides for the assessment of strategy-purity relative to an unconstrained model that is neither strategy-pure nor strategy-mixed. Here, we include a mixture model that is explicitly strategy-mixed.

Figure\ \@ref(fig:simple-model-fig) graphically depicts the core of the models. The top row shows three strategy-pure models. Panel A is the most simple strategy-pure model. All people have a true null effect, and this null effect is indicated by the spike at zero. Panel B shows another simple strategy-pure model. Here, all people have the same true positive effect. Panel C shows a strategy-pure case with individual variability. Here, individuals' effects follow a distribution over positive values rather than a spike at one value. The distribution is restricted to positive values, and it is this restriction that defines the strategy purity. The bottom row shows two strategy-mixed models. Panel D is a mixture model: People either have no effect with a certain probability or they have a positive effect that follows a distribution with a complementary probability. Models of this type are called spike-and-slab models [@George:McCulloch:1993; @Mitchell:Beauchamp:1988], with the spike referencing the point mass at zero and the slab referencing the distribution. People who are truly in the spike exhibit a different strategy than those who are truly in the slab. Finally, Panel E shows the usual random-effects model where individuals' true effects follow a normal distribution. Even though the model has a convenient mathematical form, it does not make a theoretical distinction between strategies on an individual level. We use this model as a none-of-the-above strategy-mixed option in cases where individual variation truly spans positive and negative effects. 

The goal here is to assess the evidence from the data for the various models in Figure\ \@ref(fig:simple-model-fig). If models in the top row are favored, then we may favor a strategy-pure account. Alternatively, if models in the bottom row are favored, then we may favor a strategy-mixed account. Fortunately, Bayesian model comparison through Bayes factors is ideal for this application.

In the next section, we provide a brief formal overview over the models depicted in Figure\ \@ref(fig:simple-model-fig). Following this, we outline informally the Bayes factor model comparison strategy and how it penalizes complexity by focussing on predictions. With the Bayes factors developed, we analyze priming and Stroop interference data. While strategy-mixed processing is rare, we can document at least one case where it occurs.

```{r simple-model-fig, fig.cap="Models for pure and mixed strategies. A./B. Pure-strategy accounts without individual variation. C. Pure-strategy account with individual variability. D. A mixed-strategy account where some individuals have no effect and others have a positive effect. E. Common random-effects model that can neither exclude the possibility of strategy-mixtures nor distinguish between strategies."}
par(mar=c(3,1,3,.5), mgp = c(2,1,0))
lay <- matrix(c(1, 1, 2, 2, 3, 3, 0, 4, 4, 5, 5, 0), nrow = 2, byrow = T)
layout(lay)

x <- seq(-8, 8, .1)

#Null
plot(x, y = rep(1,length(x)), pch = NA, axes = F
     , ylab = "Strategy-pure", xlab = "", ylim = c(0, 1))
axis(1, at = c(-6, 0, 6), labels = c("-", 0, "+"))
abline(v = 0, lty = "dashed", col = "gray")
Arrows(x0 = 0, x1 = 0, y0 = 0, y1 = .9
       , arr.type = "triangle", lwd = 2, arr.length = .3)
mtext("A.", line = .5)
mtext("Strategy-pure", side = 2, line = -.5, cex = .8)
mtext("Strategy-mixed", side = 2, line = -.5, adj = -4, cex = .8)


#Common
plot(x, y = rep(1,length(x)), pch = NA, axes = F
     , ylab = "", xlab = "", ylim = c(0, 1))
axis(1, at = c(-6, 0, 6), labels = c("-", 0, "+"))
abline(v = 0, lty = "dashed", col = "gray")
Arrows(x0 = 2, x1 = 2, y0 = 0, y1 = .9
       , arr.type = "triangle", lwd = 2, arr.length = .3)
mtext("B.", line = .5)

#Positive
plot(x, y = rep(1,length(x)), pch = NA, axes = F
     , ylab = "", xlab = "", ylim = c(0, 1))
axis(1, at = c(-6, 0, 6), labels = c("-", 0, "+"))
abline(v = 0, lty = "dashed", col = "gray")
lines(x, dgamma(x, shape =2), type = "l", lwd = 2)
mtext("C.", line = .5)

#SS
plot(x, y = rep(1,length(x)), pch = NA, axes = F
     , ylab = "Strategy-mixed", xlab = "", ylim = c(0, 1))
axis(1, at = c(-6, 0, 6), labels = c("-", 0, "+"))
abline(v = 0, lty = "dashed", col = "gray")
Arrows(x0 = 0, x1 = 0, y0 = 0, y1 = .7
       , arr.type = "triangle", lwd = 2, arr.length = .3)
lines(x, dgamma(x, shape =2) * .7, type = "l", lwd = 2)
mtext("D.", line = .5)

#Normal
plot(x, y = rep(1,length(x)), pch = NA, axes = F
     , ylab = "", xlab = "", ylim = c(0, 1))
abline(v = 0, lty = "dashed", col = "gray")
axis(1, at = c(-6, 0, 6), labels = c("-", 0, "+"))
lines(x, dnorm(x, sd = 1.5), type = "l", lwd = 2)
mtext("E.", line = .5)
```

# Models of constraints

```{r model-predictions, child="figures/figModPred.Rmd", cache = T}
```

The tasks we consider here have two conditions that can be termed *compatible* and *incompatible*, or more generally, *control* and *treatment*. It is most convenient to discuss the models in random-variable notation. We start with a basic linear regression model. Let $Y_{ijk}$ denote the response time (RT) for the $i$th participant, $i = 1, \ldots, I$, in the $j$th condition, $j = 1, 2$, and the $k$th trial, $k = 1, \ldots, K_{ij}$.[^1] The linear regression model is

\begin{equation}\label{basemodel}
Y_{ijk} \sim \mbox{Normal}(\alpha_i + x_j\theta_i, \sigma^2).
\end{equation}

Here, $\alpha_i$ is each individual's true intercept and $\theta_i$ is each individual's true effect. The term $x_j$ is an indicator for the condition, which is zero for compatible trials and one for incompatible trials. The parameter $\sigma^2$ is the variance of repeated trials within a cell. The critical parameters in the model are the true individuals' effects, $\theta_i$. Placing constraints on these effect parameters results in the models depicted in Figure\ \@ref(fig:simple-model-fig).

###Null Model

The null model is denoted as $\calM_0$ and specifies a true effect of zero for all individuals:

\[
  \begin{array}{llr}
\calM_0: && \theta_i = 0.\\
\end{array}
\]

This null model is more constraining than the usual null where the average across individuals is zero. Here, in contrast, each individual truly has no effect. An illustration of the model is shown in the first panel in  Figure\ \@ref(fig:modelfig). The figure illustrates the dimensionality of the models for two participants, and it is a guide useful for the following models. Shown are two hypothetical participants' true effects, $\theta_1$ and $\theta_2$, shown in the figure. For the null model, $\theta_1$ and $\theta_2$ have to be exactly zero. As a result, the density of the distribution of $\theta_i$ is a spike at zero, corresponding to the dark point at zero in the figure. The model also corresponds to Figure\ \@ref(fig:simple-model-fig)A.

###Common-effect Model

The common-effect model, denoted $\calM_1$, corresponds to the spike in Figure\ \@ref(fig:simple-model-fig)B, and it is less constrained than the null. Individuals share a common effect with no individual variability,

\[
  \begin{array}{llr}
\calM_1: && \theta_i = \nu^+,\\
\end{array}
\]

where $\nu^+$ denotes a constant, positive effect. The first panel in the second row of Figure\ \@ref(fig:modelfig) shows that both $\theta_1$ and $\theta_2$ are restricted to the diagonal line, depicting that individual participants' effects have to be equal. The diagonal is restricted to be positive to ensure that the model only accounts for effects in the expected direction. Every individual still has the exact same true effect, but this effect is only restricted to be positive, not fixed to a specific value.

###Positive-Effects Model

The positive-effects model is denoted $\calM_+$, and it is the first model that introduces true individual variability. Even so, the model still specifies strategy-purity. True individuals' effects may vary, but are constrained to be positive:

\[
  \begin{array}{llr}
\calM_+: && \theta_i \sim \mbox{Normal}^+(\nu, g_\theta \sigma^2),\\
\end{array}
\]

where $\mbox{Normal}^+$ refers to a normal distribution truncated below at zero, $\nu$ is the mean parameter for this distribution and $g_\theta \sigma^2$ is the variance term. The model is illustrated in the first panel of the third row of Figure\ \@ref(fig:modelfig).[^2] Both $\theta_1$ and $\theta_2$ are restricted to be positive, but can be different. Values closer to zero are more plausible. The model roughly corresponds to Figure\ \@ref(fig:simple-model-fig)C. In both cases, the distribution on $\theta_i$ is restricted to positive values. Yet, the shape in the figure is different from the one for the positive-effects model specified here.

###Spike-and-slab model

The spike-and-slab model is denoted $\calM_{SS}$. Here, the distribution on $\theta_i$ consists of two components, the spike and the slab. Whether an individual's effect is truly in the slab or in the spike is indicated by the parameter $z_i$. If an effect is truly null, $z_i = 0$; if an effect is truly positive $z_i = 1$. The distribution of $\theta_i$ conditional on $z_i$ is

\begin{align*}
\calM_{SS}:
\qquad
\begin{array}{l}
\theta_i | (z_i = 1) \sim \mbox{Normal}^+(\nu, g_\theta \sigma^2),\\
\theta_i | (z_i = 0) = 0,
\end{array}
\end{align*}

Here, the spike corresponds to the null model and the slab corresponds to the positive-effects model. In model specification, every individual has some probability of being in the spike and a complementary probability of being in the slab. The first panel in the fourth row of Figure\ \@ref(fig:modelfig) shows the model specifications for two participants. For these hypothetical individuals, four compinations of true effects are plausible: 1. Both individuals are in the spike. In this case, $\theta_1$ and $\theta_2$ have to be zero, indicated in the figure by the dark point at (0,0). 2. Both participants are in the slab. $\theta_1$ and $\theta_2$ can take on any positive value, restricting the true effects to the upper right quadrant in the figure., just as with the positive-effects model. 3. $\theta_1$ is in the slab and $\theta_2$ is zero. This case is represented by positive $\theta_1$ values on the horizontal line at $y = 0$. 4. $\theta_2$ is in the slab and $\theta_1$ is zero. This case is represented by positive $\theta_2$ values on the vertical line at $x = 0$.

###Unconstrained Model

The unconstrained model, denoted $\calM_u$, is the random-effects model in Figure\ \@ref(fig:simple-model-fig). Here, a normal distribution without any constraint is placed on the individual's true effects:

\[
  \begin{array}{llr}
\calM_u: && \theta_i \sim \mbox{Normal}(\nu, g_\theta \sigma^2).\\
\end{array}
\]

The first panel in the last row of Figure\ \@ref(fig:modelfig) shows these model specifications. True individuals' effects can take on any values, and values closer to zero are more plausible. With this model, there is no explicit way of taking differences in strategies into account.


## Prior specifications and hierarchical constraints

The five models are analyzed in a Bayesian framework. Bayesian analysis requires a careful specification of prior distributions on parameters. These priors are needed for parameters $\alpha_i$, the individual intercepts; $\sigma^2$, the variance of responses in each participant-by-condition cell; the collection of $z_i$, each individual's indicator of being in the spike or the slab; $\nu$, the mean of effects; and $g_\theta$, the variance of effects in effect-size units. The priors parameters that are common to all models are not of particular concern. They do not affect model comparison, and we follow @Haaf:Rouder:2017 in specification.[^z_i] Several of the models ascribe individual differences across true effects. In this regard, individuals should be treated as random, and a hierarchical treatment is appropriate [@Rouder:Lu:2005; @Rouder:etal:2008a; @Lee:2011]. We model individual differences as coming from either a normal or truncated normal with free mean and variance parameters. Prior settings on these parameters, $\nu$ and $g_\theta \sigma^2$, may affect inference. 
In the following, we describe the reasons for this influence. We show the effects of reasonable ranges of prior settings on these two parameters in the Discussion.

The shared mean parameter, $\nu$, induces correlation between the individuals' effects. Take, for example, the unconstrained model. We can recast the model on $\theta_i$ as $\theta_i = \nu + \epsilon_i$, where $\nu$ remains the population mean and $\epsilon_i \sim \mbox{Normal}(0, g_\theta \sigma^2)$ is the independent residual variation specific to an individual. The parameter $\nu$ is not given. It must be estimated. It has variability in this regard and this variability induces a correlation between individuals' effects. We take this variation into account by computing a marginal model on $\theta_i$. The marginal models are shown in the second column of Figure\ \@ref(fig:modelfig). For the the unconstrained model and the other models that specify variability, the correlation is apparent in the figure. This correlation induces dependency between $\theta_i$s, and the resultant of this dependency is a reduction in the dimensionality of the models. This reduction makes the unconstrained model, for example, more similar to the common-effect model which is important for model comparison.

[^z_i]: An exception are prior settings on $z_i$, the indicators of whether an individual is truly in the spike or the slab. We set $z_i \sim \mbox{Bernoulli}(\rho)$, where $\rho$ is the probability of being in the slab. We placed a hierarchical prior on $\rho \sim \mbox{Beta}(a, b)$, where $a = b = 1$. These prior settings represent an equal prior probability of being in the spike or the slab, and changing them may influence model comparison greatly. For this application, we decided not to explore other settings, because we do not have any theoretical implications of higher slab or spike prior probability. 

## Estimation model

The above five models describe possible constraints on individuals' effects. Assessing how applicable these models are to data is the core means to determining whether a task is strategy-pure or strategy-mixed. In the next section, we discuss a formal inferential approach---Bayes factors--- for model comparison. Even though model comparison is the main target, estimating parameters and visualizing them remains a tool for understanding structure in data. When constructing an estimation model here, we have two goals: One is to have relatively few constraints on the parameters; the second is to respect the possibility of true null effects. To meet these goals, we place a generalized spike-and-slab model on $\theta_i$.

The model has a spike at zero and a normal distribution as slab. It may be viewed as a mix between panel A and panel E in Figure\ \@ref(fig:simple-model-fig). The distribution of each individual's effect, $\theta_i$, is

\begin{align*}
\theta_i | (z_i = 1) &\sim \mbox{Normal}(\nu, g_\theta \sigma^2),\\
\theta_i | (z_i = 0) &= 0.\\
\end{align*}

This spike-and-slab model, just as the unconstrained model in Figure\ \@ref(fig:simple-model-fig), cannot distinguish between strategies. It is, however, appropriate for estimating posterior spike and slab probabilities and the collection of $\bftheta$.

<!-- Footnotes: -->
[^1]: Due to data cleaning, variation in the number of trials per person and condition is possible.

[^2]: For illustration, mean and variance of the slab are set to fixed values at $\nu = 0$ and $g_\theta \sigma^2 = .07^2$ (in seconds).

# Evidence for constraints

In the previous sections we develop five models, the null model, the common-effect model, the positive-effects model, the spike-and-slab model, and the unconstrained model that embed various meaningful constraints. Of these five, the spike-and-slab and unconstrained models are strategy-mixed cases; the null, common-effect and positive-effects models are stratgy-pure cases. Here, we provide a discussion on how to state evidence for these five models in the Bayesian framework. Rather than providing a formal discourse, which may be found in @Jeffreys:1961, @Kass:Raftery:1995, and @Morey:etal:2016, we provide an informal discussion that we have previously presented in @Rouder:etal:2016b and @Rouder:etal:2017.  Informally, evidence for models reflects how well they predict data.

The right column of Figure\ \@ref(fig:modelfig) shows predictions for data from each of the five models.  These predictions are for observed effects, $\hat{\theta}$, for each of the two exemplary participants. Note that predictions are defined on data while model specifications are defined on true effects, and this difference is reflected in the plotted quantities in the figure. For the null model, for example, *true* effects, left column, have to be exactly zero, and the *observed* effects, right column, are predicted to be near (0,0). The predictions are affected by sample noise, inasmuch as sample noise smears the form of the model.[^conv]  The remaining rows of Figure\ \@ref(fig:modelfig) show the predictions for the common-effect, positive-effects, spike-and-slab, and unconstrained models. In all cases, the precictions are smeared versions of the models.

[^conv]: More technically, the predictions are the integral $\int_{\bm{\theta}} f(\bm{Y}|\bm{\theta})\pi(\bm{\theta})d\bm{\theta}$ where $f(\bm{Y}||\bm{\theta})$ is the probability density of observations conditional on parameter values and $\pi(\bm{\theta})$ is the probability density of the parameters.

Once the predictions are known, model comparison is simple.  All we need to do is note where the data fall.  The red dots in the right column of Figure\ \@ref(fig:modelfig) denote hypothetical observed participants' effects.  These observed effects, 40 ms for participant 1 and 60 ms for participant 2, are both positive and about equal, and we might suspect that the common-effect model does well. To measure how well, we note the density of the prediction at the observed data point.  The densities for the models have numeric values, and we may take the ratio to describe the relative evidence from the data for one model vs. another.  For example, the best fitting model in the figure, the common-effect model, has a density that is three times the value of that of the unconstrained model.  Hence, the data are predicted three times as accurately under the common effect model than under the unconstrained model. This ratio is the *Bayes factor*, and it serves as the principled measure of evidence for one model compared to another in the Bayesian framework.  

Bayes factors are conceptually straightforward---one simply computes the predictive densities at the observed data.  Nonetheless, this computation is often inconvenient in practice.  It entails the integration of a multidimensional integral which is often impossible in closed form and may be slow and innacurate with numeric methods. We follow here the development by @Haaf:Rouder:2017 who provide the details of model comparison between the null, common-effect, positive-effects and unconstrained models. Their development builts on analytical solutions pioneered by @Zellner:Siow:1980 and expanded for ANOVA by @Rouder:etal:2012. These analytical solutions are used for comparisons among the null, common effect, and unconstrained models.  The development also employs the *encompassing approach* introduced by @Klugkist:etal:2005. This approach may be used for comparisons with the positive-effects model.

New to this paper are model comparisons with the spike-and-slab model. Although the spike-and-slab model is precedented and popular, we are unaware of any prior development for comparing it as a whole to alternatives. Our approach is a straightforward application of the encompassing approach. The Bayes factor between the null model and the spike-and-slab model is given by

\[
B_{0 SS} = \frac{P(\bm{z} = \bm{0}|\bfY,\calM_{SS})}{P(\bm{z} = \bm{0}|\calM_{SS})},
\]

where the event $\bm{z} = \bm{0}$ indicates that every individual is in the spike. This Bayes factor is the posterior probability that all individuals are in the spike relative to the prior probability of the same event. The same approach can be used for comparing the spike-and-slab model to the positive-effects model, using the posterior and prior probabilities that every individual is in the slab.

Computation of these probabilities is straightforward in MCMC sampling. Let $\bm{z}[m]$ denote a vector of $i$ samples of $z$ (one for each individual) on the $m$th iteration under the spike-and-slab model. The $m$th iteration is considered evidential of the null model if all $I$ elements of $\bm{z}[m]$ are zero, that is, on this iteration, every individual's effect $\theta_i$ is sampled from the spike. Let $n_{01}$ be the number of evidential iterations from the posterior, and let $n_{00}$ be the number of evidential iterations from the prior. Then, the Bayes factor is

\[
B_{0 SS} = \frac{n_{01}}{n_{00}}.
\]

To compute the Bayes factor of the spike-and-slab model to the
remaining models, we use the well-known transitivity of Bayes factors [@Rouder:Morey:2012].

# Application

```{r child = "chapters/application.Rmd"}
```

# Discussion

In this paper, we address whether people use differing strategies for tasks where the sign of the behavioral outcome measure maps well into different processing  The example we use here is priming, and we trichotomize the outcome into three basic relations: responses to  congruent targets are faster than to incongruent ones (positive priming), responses to congruent targets are slower than to incongruent ones (negative priming), and responses to congruent targets are equally fast as to incongruent ones (no priming).  Whenever a behavioral outcome can be trichotomized this way, we can assess whether processing is strategy-pure or strategy-mixed.  Obvious applications include context effects (e.g., Stroop, flanker, Simon etc.) and strength effects (e.g., stimulus strength, mnemonic strength, etc.).    

The approach we take here is Bayesian model comparison across five models: a strategy-pure null model; a strategy-pure common effect model; a strategy pure slab model; a strategy-mixed spike-and-slab model; and a strategy-mixed slab model.  The novel element here is the usage of the spike-and-slab model.  Although spike-and-slab models are used frequently in statistics, they are used to categorize which covariates (people in our case) are in the spike and which are in the slab.  Our usage is novel---we ask how well this spike-and-slab structure predicts the data relative to the other models.  

Several psychologists have previously asked the related question of whether mixtures account for data.  In cognitive psychology, the most common application is whether responses on trials are mixtures of two bases.  @Falmagne:1968 was perhaps the earliest to formally explore this notion.  He asked whether response times for a given individual are the mixture of a stimulus-driven process and a guessing process.  Indeed, this type of query has been explored in a number of domains [e.g. @Yantis:etal:1991; @Province:Rouder:2012; @Klauer:Kellen:2010].  

Our approach differs markedly from these previous queries.  Our focus is not on characterizing trial-by-trial variability but on variability across individuals.  We do not make as detailed commitments to specific cognitive architectures, but provide a general approach based on ordinal relations of less-than, same-as, and greater-than.  In this regard, our approach is more similar to latent class models used in structural equation modeling [@Skrondal:Rabe-Hesketh:2004].  In these models, vectors of outcome measures are assumed to come from the mixture of latent classes of people, and the goal is to identify the classes and categorize people into these classes.  One critique of this approach is that the models are so weakly identified that it is difficult to reliably recover class structure [@Bauer:Curran:2003].  We avoid this problem by restriction.  We restrict our classes into three that are well defined as the sign of the outcome measure.  In summary, while our approach is similar in some regards to previous, the statistical development is novel in critical ways.

We apply this approach to three exemplary data sets and find, at least for one case, some support for the mixed-strategy claim. We think, however, that mixtures of strategies are relatively rare in cognitive psychology where experimental paradigms are relatively well defined. Only in cases where tasks are ill-defined, i.e. participants have the degrees of freedom to decide on a strategy that was not anticipated by the researchers, mixtures may occur. This was the case in our location Stroop example, where participants were able to avoid reading the target words by fixating the center of the screen and still successfully completing the task. 

##Concerns

### Normal Specification

One concern with the proposed approach is the reliance on normal parametric model specifications.  The advantage of the normal specification is computational convenience.  With it, the many dimensions of the high-dimension integrals that define the Bayes factor may be computed symbolically to high precision.  Without it, we suspect numeric integration would be exceedingly slow and inaccurate.  Yet, researchers may be concerned about the misspecification of the normal.  Here, for example, we focus on applications with response times. RT is skewed rather than symmetric, and the standard deviation tends to increase with the mean [@Wagenmakers:Brown:2007; @Luce:1986; @Rouder:etal:2010d].  

We think this concern is misplaced.  The main reason is that we focus on the analysis of ordinal relations among true means.  If we knew individual's true means, then we could answer the processing questions without any need to know or consider the true shapes or true variances.  The inference here therefore inherently has all the robustness of ANOVA or regression, which is highly robust for skewed distributions, so long as the left tail is thin.  Indeed, RTs tend to have thin left tails that fall off no slower than an exponential [@Burbeck:Luce:1982; @VanZandt:2000 @Wenger:Gibson:2004].  

@Thiele:etal:2017 addressed this concern through simulation.  They considered highly similar models and performed inference with similarly computed Bayes factors.  In simulation, they generated data from a shifted log normal with realistic skewness and with means and variances that varied across individuals and the manipulation.  As expected, they found exceptional robustness, and the reason is clear.  The main inferential logic is dependent only on true means, and the normal is a perfectly fine model for assessing this quantity even when the data are not normally distributed.

### Prior Sensitivity

```{r prior-sensitivity, child="figures/senstab.Rmd", cache = F}
```

Another concern, perhaps a more pressing concern in our view, is understanding the role and effects of the prior on inference.  In general, Bayesian models require a careful choice of priors. These priors have an effects on inference as noted by many Bayesians.  A general idea in research is that, if two researchers run the same experiment and obtain the same data, they should reach the same if not similar conclusions.  Yet, the priors may be chosen differently by different researchers, and this choice may lead to differing conclusions.  To harmonize Bayesian inference with the above starting point, many Bayesian analysts actively seek to minimize these effects by picking likelihoods, prior parametric forms, and heuristic methods of inference so that variation in prior settings have marginal effects [@Aitkin:1991;@Gelman:etal:2004;@Kruschke:2012;@Spiegelhalter:etal:2002]. In contrast, @Rouder:etal:2016b argue that the goal of analysis is to add value by searching for theoretically-meaningful structure in data.  @Vanpaemel:2010 and @Vanpaemel:Lee:2012 argue that the prior is where theoretically important constraint is encoded in the model.  In our case, the prior provides the critical constraint on the relations among individuals.  We think it is best to avoid judgments that Bayes factor model comparisons depend too little or too much on priors.  They depend on it to the degree they do.

Here we focus on understanding the dependence of Bayes factors on a reasonable range of prior settings and the resulting diversity of opinions. Indeed, @Haaf:Rouder:2017 took this tactic in understanding the diversity of results with all the models except for the spike-and-slab-model which was not developed at the time. Here we use a similar range of prior settings to understand the dependency on these settings.  

The critical prior settings for understanding the diversity of conclusions come from the priors on $\nu$ and $\epsilon_i$ (or $g_\theta$).  Although they are not the primary target of inference, the prior settings on these parameters do affect Bayes factor results.  A full discussion of the prior structures on these parameters is provided in @Haaf:Rouder:2017, and here we review the main issues.  The critical settings are the *scales* on $\nu$ and $\epsilon_i$, and these settings are relative to $\sigma$, the residual noise.  In tasks like this, with subsecond RTs, a standard deviation of repeated response times for a given participant and a given condition is about 300 ms, and we can use this value to help set the scales. For example, for priming and Stroop tasks, we may expect an overall effect of 50ms, and the scale on $\nu$ might be $1/6$th or $50/300$. Likewise, if the take the variability of individuals' effects depicted by $\epsilon_i$, we may expect this variation to be about 30 ms, or $1/10$ of the residual noise. We explore the effects of halving and doubling these settings, which represents a reasonable range of variation. 

With these reasonable ranges of variation, we are ready to explore the effects of prior specification on Bayes factors. These are shown in Table \@ref(tab:sensitivity-tab). There is a fair amount of variability in Bayes factors, and in our opinion, there should be. The range of settings define quite different models with quite different predictions. Nonetheless, there is a fair amount of consistency. For the priming data, the common-effect model is preferred for all settings, with the null-model and the spike-and-slab models as the next contenders. For the color Stroop data, the positive-effects model is preferred for all settings, and the ordering for the remaining models stays relatively constant. The only data set where the preferred model varies with prior settings is the location Stroop data: The spike-and-slab model is preferred for the chosen settings and when the scale non $\nu$ is halved. These settings indicate that small average effects are expected for all models. When the scale on $\nu$ is doubled, i.e. larger, about 100ms effects are expected, the Bayes factor between the common-effect model and the spike-and-slab model is about 1, indicating that none of the two models is preferred over the other. This Bayes factor, however, was not extensively large from the beginning, only about 3-to-1 in favor of the spike-and-slab model. This example illustrates how useful this type of sensitivity analysis can be to understand the range of conclusions that may be drawn from the data. In this case, the evidence for the spike-and-slab model is small, and largely dependent on prior settings. For a convincind result, more evidence for a mixed-strategy account would is needed.

### Computational Issues

In previous work [@Haaf:Rouder:2017; @Thiele:etal:2017] we developed the null, common-effect, positive-effects and unconstrained models. Here we add the spike-and-slab model to the set and show it is a worthy competitor in at least one application. The former four models are computationally convenient, the Bayes factors can be computed quicky using a combination of Rouder et al.'s [-@Rouder:etal:2012] symobic integration as implemented in the BayesFactor package for R [@R-BayesFactor] and Klugkist and colleagues encompassing approach [e.g. @Klugkist:Hoijtink:2007]. Bayes factors for the spike-and-slab model, however, while conceptually similar, is computationally far more difficult in practice. The difficulty here is that Bayes factor computation is reliant on Marcov-chain-Montecarlo methods. We find that convergence in these methods is slow for the spike-and-slab model and hundreds of thousands of iterations are needed to approximate the Bayes factor. The good news here is that we can assess the accuracy of this approximation fairly readily through transitivity of Bayes factors. Figure\ \@ref(fig:result-fig)B/D/F illustrate this check. We can compute the Bayes factor between the unconstrained model and the null model either through the spike-and-slab model directly with symbolic integration methods. We find comparable results from both methods.

Our computational difficulties illustrate that there is much work to be done. Although Bayes factors are convenient in standard cases with normal distributions, moving to the assessment of more custom-tailored models of psychological process remains timely and topical. 

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
