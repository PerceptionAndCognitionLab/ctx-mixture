
A prevailing folk wisdom is that different people do things differently. 
And in psychological science, the study of individual differences has a long and storied tradition [@Cattell:1946].  A modern target of inquiry has been to understand the covariation of individuals' performance in common cognitive tasks that tap perceptual, attention, and mnemonic abilities [e.g. @Miyake:etal:2000]. In the usual course of studying these relationships, individuals are almost always considered as coming from a continuous, graded distribution, most often the normal [e.g., @Bollen:1989]. 

Yet, the tasks that researchers study often have a natural zero point.
Take for example, a common number-priming task [@Naccache:Dehaene:2001; @Pratte:Rouder:2009]. 
Here, participants are asked to determine whether a target digit is greater than or less than five.  Before participants see the target digit, they are exposed to a briefly flashed prime.  This prime is another digit, also greater than or less than five. The prime may be *congruent* with the target, i.e.; both numbers are less than (greater than) five, or it may be *incongruent* with the target, where each has a different status from the number five.  People respond more slowly to targets when the prime is incongruent than congruent, and this slow-down defines the priming effect.  Note that there is a natural zero point where there is no priming, that is responses to targets are the same for congruent and incongruent primes.  A positive priming effect occurs when responses to targets following congruent primes are faster than those following incongruent primes; a negative priming effect occurs when responses to targets following congruent primes are slower than those following incongruent primes.  Here, negative and positive priming effects are qualitatively different because they lead to different theoretical implications.  A positive priming effect may indicate the presence of response activation from the prime [@Eriksen:Eriksen:1974].  A negative priming effect may indicate temporal segregation and suppression [@Dixon:DiLollo:1994].  

Cognitive psychologists are well aware of the importance of the qualitative and theoretical distinction between positive and negative effects.  When experimentalists can find manipulations that switch an effect from positive to negative, the switch itself becomes a target of study. For example, @Eimer:Schlaghecken:2002 show a reversal of the priming effect. In their Experiment 2, they manipulate the prime presentation duration. For longer presentation durations, say between 60 ms and 100 ms, the regular, positive priming effect is observed. For shorter presentation durations, say between 16 ms and 50 ms, the authors document a negative priming effect. The combination of results are interpreted as evidence for two different priming processes: a subliminal process leading to inhibition, and a supraliminal process leading to facilitation of the primed response. A second example for the reversal of an effect comes from @Rouder:King:2003. The authors reversed the usual Eriksen-flanker effect by using morphed targets.  When targets were morphed between two letters, there was a contrast effect where responses to targets incongruent with their surrounds were speeded. When clear letters were used, there was the typical assimilation effect where responses to incongruent targets was slowed down. @Rouder:King:2003 interpreted the combination of results as implicating two separate and opposing effects in perception and response selection.

Although these examples show that it may be possible to reverse specific effects in tightly controlled contexts, doing so remains quite rare.  To our knowledge, nobody has reversed the Stroop effect where responses are faster to incongruent items than to congruent ones.  Likewise, it is unlikely that a strength effects can be reversed----that is, we doubt, for example, that there are conditions were responses may be quicker to dim lights than to bright ones. And because these phenomena seemingly cannot be reversed, simple explanations are warranted.  In the strength case, it may indicate a rather direct link between stimulus strength and neural activation without much opportunity for modulation from top-down processes. We suspect for a wide class of phenomena, the zero point is never crossed [@Haaf:Rouder:2017].

These examples, where zero is crossed and where it is not, show the theoretical importance of the zero point in understanding cognition.  Yet, considerations of individual differences, where individuals' abilities are assumed to come from a graded distribution, do not respect this importance.  If an analyst assumes say that individuals' true Stroop abilities come from a normal, then some people by definition are assumed to have a truly negative Stroop effect. What a fantastic finding that would be! The problem with graded distributions that cross the zero point is they deny the possibility that true negative effects do not occur. In doing so, they miss the possibility of important global constraints like the impossibility of negative Stroop effects.^[This negative Stroop effect is not to be confused with the *reversed Stroop effect* [e.g. @Logan:Zbrodoff:1979], where participants are asked to respond to the word instead of the ink color of the word.]

Another problem with graded distributions of individuals' effects is that they explicitly assume that no individual has a true effect of exactly zero.^[By definition, the probability of any point in a continuous distribution is identically zero.]  Yet, the idea that some individuals do not display an effect is compelling from a theoretical point of view.  Are there effects where some people are immune, and if so, how could we tell?

The main goal of this paper is to develop models that encompass several configurations of individual variability. Do individuals follow a graded distribution? Are there order-constraints such that nobody can have a negative effect? And are there individuals with truly no effect?  The work builds on our previous development where individuals could be all positive, all negative, all null, or follow the traditional graded normal distribution [@Haaf:Rouder:2017; @Thiele:etal:2017].  None of our previous work, however, captures the notion that "some do and some don't," that is that some individuals show a positive effect while others show no effect.  

Previously, researchers have attempted to solve this problem with classification: They assume there are do-ers and don't-ers, and classify people as such.  We show next why this approach does not work well.  Then we develop our models and apply them to a number of extant context and priming effects.  The results are quite surprising---we show that there are cases where "everyone does" and other cases where "some do and some don't."  We have yet to find a case where some people have a negative effect while others have a positive effect.  Perhaps this unicorn is out there, and the tools developed here define the state-of-the-art for finding it.

To understand the nature of the problem and the solution, it is critical to distinguish between observed and true effects.  Take, for example, data from a priming task shown in Figure\ \@ref(fig:classification).^[Data comes from @Pratte:Rouder:2009, Experiment 2. Details on the data set may be found in the Application section.] 
Plotted are each individual's *sample effect*---that is, the difference between the mean response for incongruent primes and congruent primes.  The sample effects are ordered from smallest to largest in the figure, and as can be seen, some are negative, other are near zero, and a few are substantially positive.  But this does not mean that any particular person has a *true effect* that is negative, near zero, or positive.  True effects are the underlying latent effects we could observe if we had infinitely many trials per individual. They are the target of interest; we wish to know whether they are negative, zero, or positive.  The sample effects reflect the true effects, but they are also perturbed by trial noise.  Hence, some analysis is needed to infer the qualitative status of the true effects.

# The argument against classification

One approach that has been developed for the above problem is to classify individuals as having negative, null and positive effects. A good example comes from @Little:etal:2011, who classified individuals as using serial, parallel or co-active processing based on the direction and magnitude of an interaction contrast. One approach to classification is to consider and individual's confidence interval (CI) around the sample effect.  The idea is that the true value is likely somewhere in the CI [cf. @Morey:etal:2016b]. We can apply this approach to the data in Figure\ \@ref(fig:classification). The bars show the 80% confidence intervals, and the key observation is whether these CIs include zero or not. We see no individuals are highly likely to have truly negative effects, some may have null effects, and a few others are squarely positive.  Of course, we need not use CIs; we can even calculate a Bayes factor per individual, and once again, we conclude that some do and some don't. 

The CI and Bayes factor approaches used in Figure\ \@ref(fig:classification) are a bit naive.  @Houpt:Fific:2017 and @Rouder:etal:2007b develop more sophisticated hierarchical versions of classification where the probability of placing any individual in a category depends in part on how the others were placed as well.  This hierarchical smoothing is an improvement, but it does not eliminate the following critique from @Lee:Webb:2005.

According to Lee and Webb [-@Lee:Webb:2005], a problem with the above classification schemes is that they start with a structural assumption.  If one starts with the assumption of different classes, then it is not surprising that one ends up with people categorized in different classes, if only by sample noise.  @Thiele:etal:2017 make a similar argument---they show that when classes are assumed, then sample noise alone leads to a propensity to find people in different classes.  In fact, with an appropriate analysis, to be presented subsequently, we show that the individual sample effects in Figure\ \@ref(fig:classification) are best understood as arising from a positive true effect with no variation across individuals.

Lee and Webb [-@Lee:Webb:2005] proposed a non-parametric Bayesian modeling approach similar in spirit to cluster analysis.  If there is a natural group structure, individuals are clustered together.  The number of clusters, their locations and their variances are estimated.  Lee and Webb's approach is noteworthy in many respects, but it does not take into account the qualitative distinctions among negative, null, and positive effects.  For example, individuals with true effects of -20 ms and 20 ms may be lumped together while those with true effects of 20 ms and 200 ms may be lumped apart.  This is problematic because if individuals truly differ in the sign of effects, a far more complicated and nuanced set of processes is implicated.  

To address this issue, @Haaf:Rouder:2017 and @Thiele:etal:2017 developed several models where each model instantiates different possible configurations of individual differences.  We extend this approach here to assess the folk wisdom that some do and some don't. Figure\ \@ref(fig:simple-model-fig) shows different possible configurations of models. Panel A shows a strong null model. All people have a true null effect, and this null effect is indicated by the spike at zero. Panel B shows a common-effect model; all people have the same true positive effect. Panel C shows a case where individuals' effects vary, but, importantly, they do not cross zero.  Moreover, there is no mass at zero, and this model is a flexible version of the "everyone does" position. Panel D shows a model where some do and some don't.  It is a mixture model, and with a certain probability individuals have no effect.  Models of this type are called spike-and-slab models [@George:McCulloch:1993; @Mitchell:Beauchamp:1988], with the spike referencing the point mass at zero and the slab referencing the positive distribution. People who are truly in the spike have no true effect and those who are in the slab have a true effect in the positive direction. 

Panel E shows a three-component mixture: some people show a true positive effect, some show a true negative effect, and some show a true null effect. This model is again a spike-and-slab model with one spike at zero and two slabs, one on each side of zero. Note that there are only few cases where this model may be theoretically implied. One such case is @Little:etal:2011, where all three components of the model may be mapped to different processing architectures.  Yet, we will not carry this model for the current analysis. One reason is that it is not theoretically useful for the applications chosen here.  A second reason is that it is computationally inconvenient (We will return to the computational difficulties in the discussion section).  A third, and perhaps most important reason, is that if this case holds, the normal model in Panel F will fare quite well.  We show this in simulation subsequently.

Panel F shows the usual case where individuals' true effects follow a normal distribution. Even though the model has a convenient mathematical form, it does not take the theoretical importance of the zero point into account. It may be used to highlight the differences between conventional approaches and our development.

The goal here is to assess the evidence from the data for the various models in Figure\ \@ref(fig:simple-model-fig).  If models in the top row are favored, then we may favor an accounts where each individual is behaving with the same strategies and processes.  Alternatively, if models in the bottom row are favored, then we may favor accounts with qualitative differences in processing and strategies among individuals. The approach is different from categorization because the goal is not to categorize individuals but to compare models of configural relations that embed important theoretical distinctions.  

From a classical perspective, the analyses of and comparison among the models in Figure\ \ref{fig:simple-model-fig} is difficult.  The analyst must account for the possible range restrictions on true effects, and doing so is known as *order-restricted inference*.  Order-restricted inference is a difficult topic in statistical analysis [@Robertson:etal:1988; @Silvapulle:Sen:2011], and we know of no tests appropriate for the comparison of the null model of Figure\ \ref{fig:simple-model-fig}A to the "everyone does" model in Figure\ \ref{fig:simple-model-fig}C.  In contrast, Bayesian model comparison through Bayes factors is conceptually straightforward and computationally convenient.  @Gelfand:etal:1992 provide the conceptual insights;  @Haaf:Rouder:2017 and @Klugkist:etal:2005 provide the computational implementations for all the models except the spike-and-slab versions.  The development of the spike-and-slab model that encodes the some do and some don't position is novel.  We are proud of the development because the model addresses an important element of folk wisdom, serves as a precursor for more advances mixture models. Moreover, we are proud to have made the analysis computationally convenient.

In the next section, we provide a brief formal overview of the models depicted in Figure\ \@ref(fig:simple-model-fig). Following this, we informally outline the Bayes factor model comparison strategy. With the Bayes factors developed, we analyze priming and Stroop interference data. We document at least one case where the some-do-and-some-don't wisdom seems to be a good description.


